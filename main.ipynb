{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Jnx4-jwi2mB8Wz8Qw5pezrujr0NR639v",
      "authorship_tag": "ABX9TyPZIYr8AeZgXxXKBG/ei2qW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobbaumgartner/CNN-HotDog-Classifier/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulg-fCtYhbP8"
      },
      "source": [
        "# %tensorflow_version 1.x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzLPSpwt-50c"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A268xpRi_VJ3",
        "outputId": "012eb959-1adc-4df9-b4c3-73ef2f91b221"
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pId__zSmALyc"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuQtse1ZBMvb"
      },
      "source": [
        "Če imam izbrano GPU:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GCyE-yoFnNhZ",
        "outputId": "86990d6e-15f4-4769-a46c-a6aeb0e39c03"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrkngRicBJEx"
      },
      "source": [
        "Če pa bi želel uporabiti TPU: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phzyD8iCAzcp"
      },
      "source": [
        "# use_tpu = True #@param {type:\"boolean\"}\n",
        "\n",
        "# if use_tpu:\n",
        "#     assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "# if 'COLAB_TPU_ADDR' in os.environ:\n",
        "#   TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "# else:\n",
        "#   TF_MASTER=''"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeqxfKaofLEH"
      },
      "source": [
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\r\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jjSaHAwIRjt"
      },
      "source": [
        "Nastavimo poti do fotografij, uredimo fotografije. In izdelamo training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOd0JkpXHHYX",
        "outputId": "e9789def-bc31-4ee8-a674-9ef1da751951"
      },
      "source": [
        "categories = [\"hot_dog\", \"not_hot_dog\"]\r\n",
        "datadir = \"drive/MyDrive/Colab Notebooks/seefood/train\"\r\n",
        "os.getcwd()\r\n",
        "IMG_SIZE = 400 # maybe change this setting later, if the results are not good\r\n",
        "\r\n",
        "training_data = []\r\n",
        "\r\n",
        "for category in categories:  # do hot_dog and not_hot_dog\r\n",
        "\r\n",
        "    path = os.path.join(datadir,category)  # create path to hotdogs\r\n",
        "    class_num = categories.index(category)  # get the classification  (0 or a 1). hot or not\r\n",
        "\r\n",
        "    for img in tqdm(os.listdir(path)):  # iterate over each image \r\n",
        "        try:\r\n",
        "            img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to grayscale array\r\n",
        "            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE)) # resize \r\n",
        "            training_data.append([new_array, class_num]) \r\n",
        "          \r\n",
        "        except Exception as e:  # in the interest in keeping the output clean...\r\n",
        "              print(e)\r\n",
        "      \r\n",
        "import random\r\n",
        "random.shuffle(training_data)\r\n",
        "\r\n",
        "X = []   # features (big X)\r\n",
        "y = []   # labels (small y)\r\n",
        "\r\n",
        "for features,label in training_data:\r\n",
        "    X.append(features) \r\n",
        "    y.append(label)   # labels\r\n",
        "\r\n",
        "print(X[0].reshape(-1, IMG_SIZE, IMG_SIZE, 1))\r\n",
        "\r\n",
        "# we have to make features into a np array\r\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # number of features, size_x, size_y, colors (grayscale = 1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 449/449 [00:02<00:00, 206.39it/s]\n",
            "100%|██████████| 449/449 [00:02<00:00, 205.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[[[0]\n",
            "   [0]\n",
            "   [0]\n",
            "   ...\n",
            "   [0]\n",
            "   [0]\n",
            "   [0]]\n",
            "\n",
            "  [[0]\n",
            "   [0]\n",
            "   [0]\n",
            "   ...\n",
            "   [0]\n",
            "   [0]\n",
            "   [0]]\n",
            "\n",
            "  [[0]\n",
            "   [0]\n",
            "   [0]\n",
            "   ...\n",
            "   [0]\n",
            "   [0]\n",
            "   [0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1]\n",
            "   [1]\n",
            "   [2]\n",
            "   ...\n",
            "   [0]\n",
            "   [0]\n",
            "   [0]]\n",
            "\n",
            "  [[1]\n",
            "   [1]\n",
            "   [2]\n",
            "   ...\n",
            "   [0]\n",
            "   [0]\n",
            "   [0]]\n",
            "\n",
            "  [[1]\n",
            "   [1]\n",
            "   [2]\n",
            "   ...\n",
            "   [0]\n",
            "   [0]\n",
            "   [0]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIxuyANhTAYM"
      },
      "source": [
        "Sedaj želimo premešati podatke, da niso vse slike kategorij po vrsti, drugače imamo težave z učenjem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWvljyivXzSb"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "pickle_out = open(\"X.pickle\",\"wb\")\r\n",
        "pickle.dump(X, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_out = open(\"y.pickle\",\"wb\")\r\n",
        "pickle.dump(y, pickle_out)\r\n",
        "pickle_out.close()\r\n",
        "\r\n",
        "pickle_in = open(\"X.pickle\",\"rb\")\r\n",
        "X = pickle.load(pickle_in)\r\n",
        "\r\n",
        "pickle_in = open(\"y.pickle\",\"rb\")\r\n",
        "y = pickle.load(pickle_in)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PzZTu5CK-T1"
      },
      "source": [
        " *The basic CNN structure is as follows: Convolution -> Pooling -> Convolution -> Pooling -> Fully Connected Layer -> Output*\r\n",
        "\r\n",
        "*Convolution is the act of taking the original data, and creating feature maps from it.Pooling is down-sampling, most often in the form of \"max-pooling,\" where we select a region, and then take the maximum value in that region, and that becomes the new value for the entire region. Fully Connected Layers are typical neural networks, where all nodes are \"fully connected.\" The convolutional layers are not fully connected like a traditional neural network.* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvKSm8nUL0WF"
      },
      "source": [
        "Na tej točki začnemo zares načrtovati nevronsko mrežo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhJZ7UQ7LCNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9c145c-5fbc-44fd-ec35-145879b4a081"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n",
        "from tensorflow.keras.callbacks import TensorBoard\r\n",
        "\r\n",
        "%load_ext tensorboard\r\n",
        "\r\n",
        "import time"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r16yedDQ7kL"
      },
      "source": [
        "X=np.array(X/255.0)\r\n",
        "y=np.array(y)\r\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIV5v_vW8yY",
        "outputId": "1b1b71d5-8b7f-4706-e95e-852a7abf78e2"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG0fJXjcLw9n"
      },
      "source": [
        "# NAME = \"hot_dog_CNN\"\r\n",
        "# tensorboard_callback = TensorBoard(log_dir=\"logs/{}\".format(NAME))\r\n",
        "\r\n",
        "# # log_dir = '/gdrive/My Drive/project/' + \"logs/fit/\"\r\n",
        "# # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n",
        "\r\n",
        "# model = Sequential()\r\n",
        "\r\n",
        "# model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\r\n",
        "# model.add(Activation('relu'))\r\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "# model.add(Conv2D(128, (3, 3)))\r\n",
        "# model.add(Activation('relu'))\r\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "# model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\r\n",
        "\r\n",
        "# model.add(Dense(64))\r\n",
        "# model.add(Activation('relu'))\r\n",
        "\r\n",
        "# model.add(Dense(1))\r\n",
        "# model.add(Activation('sigmoid'))\r\n",
        "\r\n",
        "# model.compile(loss='binary_crossentropy',\r\n",
        "#               optimizer='adam',\r\n",
        "#               metrics=['accuracy'])\r\n",
        "\r\n",
        "# model.fit(X, y, \r\n",
        "#           batch_size=8, \r\n",
        "#           epochs=16,\r\n",
        "#           verbose = 1, \r\n",
        "#           callbacks=[tensorboard_callback],\r\n",
        "#           validation_split=0.1\r\n",
        "#           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b2t75C-uwc"
      },
      "source": [
        "Sedaj treniramo model in spreminjamo parametre, da se poizkusimo izogniti overfiting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8JxiZZxAytG"
      },
      "source": [
        "NAME = \"hot_dog_CNN\"\r\n",
        "tensorboard_callback = TensorBoard(log_dir=\"logs/{}\".format(NAME))\r\n",
        "\r\n",
        "# log_dir = '/gdrive/My Drive/project/' + \"logs/fit/\"\r\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "model.add(Conv2D(128, (3, 3)))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\r\n",
        "\r\n",
        "model.add(Dense(64))\r\n",
        "model.add(Activation('relu'))\r\n",
        "\r\n",
        "model.add(Dense(1))\r\n",
        "model.add(Activation('sigmoid'))\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer='adam',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(X, y, \r\n",
        "          batch_size=8, \r\n",
        "          epochs=16,\r\n",
        "          verbose = 1, \r\n",
        "          callbacks=[tensorboard_callback],\r\n",
        "          validation_split=0.1\r\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBHp6M_zgjp4"
      },
      "source": [
        "%tensorboard --logdir logs  # show tensorBoard "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTWziST8LyaO"
      },
      "source": [
        ""
      ]
    }
  ]
}